{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcAw31JoqeTvW0yGFKg3cy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amiralitalebi/CN6005_2321004/blob/main/Week10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLz8wf2OtO5j",
        "outputId": "512c8366-f978-48e1-f943-b24da2cbd0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.11.12)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install textblob vaderSentiment\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "text = \"Python is such an amazing language!\"\n",
        "score = sid.polarity_scores(text)\n",
        "\n",
        "print(\"Text:\", text)\n",
        "print(\"VADER sentiment scores:\", score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HYC1HLCtXP5",
        "outputId": "21026577-a12a-457c-dc67-c92aa025f56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Python is such an amazing language!\n",
            "VADER sentiment scores: {'neg': 0.0, 'neu': 0.55, 'pos': 0.45, 'compound': 0.6239}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"Python is such an amazing language!\"\n",
        "analysis = TextBlob(text)\n",
        "\n",
        "print(\"Text:\", text)\n",
        "print(\"Polarity and subjectivity:\", analysis.sentiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl2HZ-HhtYJL",
        "outputId": "18e08f48-bc4d-44e0-d7b3-643501c67cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Python is such an amazing language!\n",
            "Polarity and subjectivity: Sentiment(polarity=0.37500000000000006, subjectivity=0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "text = \"Python is such an amazing language!\"\n",
        "sentiment_score = analyzer.polarity_scores(text)\n",
        "\n",
        "print(\"Text:\", text)\n",
        "print(\"VaderSentiment scores:\", sentiment_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncnUYtlYtZZJ",
        "outputId": "bb9eb798-edf4-4575-9825-5fad5c3845f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Python is such an amazing language!\n",
            "VaderSentiment scores: {'neg': 0.0, 'neu': 0.55, 'pos': 0.45, 'compound': 0.6239}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"The cat is on the mat.\",\n",
        "    \"The dog is in the house.\",\n",
        "    \"The mat is in the house.\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "for doc_index, doc in enumerate(tfidf_matrix.toarray()):\n",
        "    print(f\"\\nDocument {doc_index + 1}:\")\n",
        "    for term_index, value in enumerate(doc):\n",
        "        if value > 0:\n",
        "            print(f\"  {feature_names[term_index]}: {value:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9KH5bgZtbUI",
        "outputId": "ad095d50-b416-401b-9ed7-e80fb553f500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1:\n",
            "  cat: 0.481\n",
            "  is: 0.284\n",
            "  mat: 0.366\n",
            "  on: 0.481\n",
            "  the: 0.568\n",
            "\n",
            "Document 2:\n",
            "  dog: 0.506\n",
            "  house: 0.385\n",
            "  in: 0.385\n",
            "  is: 0.299\n",
            "  the: 0.598\n",
            "\n",
            "Document 3:\n",
            "  house: 0.408\n",
            "  in: 0.408\n",
            "  is: 0.317\n",
            "  mat: 0.408\n",
            "  the: 0.633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "data = [\n",
        "    (\"I love this product\", \"positive\"),\n",
        "    (\"This is terrible\", \"negative\"),\n",
        "    (\"I am not sure about this\", \"neutral\"),\n",
        "    (\"Absolutely fantastic, I am very happy\", \"positive\"),\n",
        "    (\"I hate this, it is awful\", \"negative\"),\n",
        "    (\"It is okay, nothing special\", \"neutral\"),\n",
        "]\n",
        "\n",
        "X, y = zip(*data)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_counts, y_train)\n",
        "\n",
        "predictions = classifier.predict(X_test_counts)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "class_report = classification_report(y_test, predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-_1KtcDtcXW",
        "outputId": "f689aaf3-85cf-4650-c73b-b4ec79580a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.00\n",
            "\n",
            "Confusion Matrix:\n",
            "[[0 1 0]\n",
            " [0 0 0]\n",
            " [0 1 0]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       1.0\n",
            "     neutral       0.00      0.00      0.00       0.0\n",
            "    positive       0.00      0.00      0.00       1.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This week I worked on understanding sentiment analysis and how it is used in real systems. At first, I thought the topic would be difficult, but when I followed the slides and the practical code, it started to make more sense. I learned that sentiment analysis is about finding the emotion in text, such as positive, negative, or neutral. I also learned two main methods: using a lexicon and using machine learning. The lexicon method was easier for me because it is rule-based and I only needed to count positive and negative words. This helped me understand the basic idea in a simple way.\n",
        "\n",
        "Doing the machine learning part in Google Colab was a new experience for me. I used CountVectorizer to turn text into numbers, and then trained a Naive Bayes model. When I saw the accuracy score and the confusion matrix, I finally understood how we check if a model is good. I also realised that having more training data would improve the results. This taught me that machine learning models depend a lot on the quality of the dataset.\n",
        "\n",
        "The main challenge I had was understanding some of the technical terms, like TF-IDF and feature extraction. To overcome this, I used the examples from the slides and ran them step by step in Colab. This helped me see what each part of the code does. I feel more confident now because I can explain the process in simple words and I can run the code without errors.\n",
        "\n",
        "Overall, this task helped me build both practical and theoretical skills. I now understand how computers analyse opinions, and I feel more prepared for future coursework in NLP and machine learning. If I continue practising and use larger datasets, I believe my skills will improve even more."
      ],
      "metadata": {
        "id": "7h_K_VPgtqHI"
      }
    }
  ]
}